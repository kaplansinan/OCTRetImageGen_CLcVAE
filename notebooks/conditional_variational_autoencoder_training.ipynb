{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd074b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import glob \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from IPython import display\n",
    "# import imageio\n",
    "import PIL\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a564a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tf version\n",
    " assert tf.__version__ == '2.4.1' , \"TF version is not matching! Make sure you have tf 2.4.1-gpu installed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enable GPU memory growth - avoid allocating all memory at start\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(device=gpu, enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52573f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions \n",
    "sys.path.append('../')\n",
    "from src.dataloader.cvae_loader import debug_batch_of_data, get_training_tfdata\n",
    "from src.models.cvae_model import CCVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e53584",
   "metadata": {},
   "source": [
    "### Define losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMatchingLoss(keras.losses.Loss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mae = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss = 0\n",
    "        for i in range(len(y_true) - 1):\n",
    "            loss += self.mae(y_true[i], y_pred[i])\n",
    "        return loss\n",
    "\n",
    "\n",
    "class VGGFeatureMatchingLoss(keras.losses.Loss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder_layers = [\n",
    "            \"block1_conv1\",\n",
    "            \"block2_conv1\",\n",
    "            \"block3_conv1\",\n",
    "            \"block4_conv1\",\n",
    "            \"block5_conv1\",\n",
    "        ]\n",
    "        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n",
    "        vgg = keras.applications.VGG19(include_top=False, weights=\"imagenet\")\n",
    "        layer_outputs = [vgg.get_layer(x).output for x in self.encoder_layers]\n",
    "        self.vgg_model = keras.Model(vgg.input, layer_outputs, name=\"VGG\")\n",
    "        self.mae = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = keras.applications.vgg19.preprocess_input(127.5 * (y_true + 1))\n",
    "        y_pred = keras.applications.vgg19.preprocess_input(127.5 * (y_pred + 1))\n",
    "        real_features = self.vgg_model(y_true)\n",
    "        fake_features = self.vgg_model(y_pred)\n",
    "        loss = 0\n",
    "        for i in range(len(real_features)):\n",
    "            loss += self.weights[i] * self.mae(real_features[i], fake_features[i])\n",
    "        return loss\n",
    "\n",
    "# KL-Divergence loss\n",
    "def kl_divergence_loss(mean, variance):\n",
    "    return -0.5 * tf.reduce_sum(1 + variance - tf.square(mean) - tf.exp(variance))\n",
    "# MSE\n",
    "MSE = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def reconstruction_loss(y_true, y_pred):\n",
    "#     mse = tf.keras.losses.MeanSquaredError()\n",
    "    return MSE(y_true,y_pred)\n",
    "\n",
    "# Perceptual loss\n",
    "vgg_loss = VGGFeatureMatchingLoss()\n",
    "\n",
    "# Feature matching loss\n",
    "feature_matching_loss = FeatureMatchingLoss()\n",
    "\n",
    "def vae_loss(inputs, outputs,z_mean,z_log_var, image_size=256):\n",
    "    beta = 1.0\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5 * beta\n",
    "    tmp_vgg_loss = vgg_loss(inputs, outputs)\n",
    "    tmp_feat_loss = feature_matching_loss(inputs, outputs)\n",
    "    cvae_loss = K.mean(25*tmp_vgg_loss + 10*tmp_feat_loss+0.1*kl_loss) # best result so far\n",
    "    return cvae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliy function to compute loss\n",
    "def compute_loss_v3(model, x_image, x_cond_labels):\n",
    "        mean, logvar = model.encode([x_image, x_cond_labels])\n",
    "        z = model.reparameterize(mean, logvar)\n",
    "        x_logit = model.decode([z, x_cond_labels])\n",
    "        return vae_loss(x_image, x_logit,mean,logvar,image_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a165519",
   "metadata": {},
   "source": [
    "### Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b85178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "epochs = 200\n",
    "# set the dimensionality of the latent space to a plane for visualization later\n",
    "latent_dim = 128\n",
    "image_shape = (256,256,3)\n",
    "num_examples_to_generate = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c74ee75",
   "metadata": {},
   "source": [
    "### Tf function for custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a721081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer \n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "# tf function for custom training loop\n",
    "@tf.function\n",
    "def train_step(model, x_image, x_cond_labels, optimizer):\n",
    "    \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "      This function computes the loss and gradients, and uses the latter to\n",
    "      update the model's parameters.\n",
    "      \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "#         loss = compute_loss(model, x_image, x_cond_labels)\n",
    "        loss = compute_loss_v3(model, x_image, x_cond_labels)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fede05",
   "metadata": {},
   "source": [
    "### Read training set of CNV cases \n",
    "In this notebook, we will train a model for CNV class only. Hence, while loading the training set paths, we will filter only CNV cases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbdc019",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path =  \"/data/oct_train_filtered.csv\" \n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae7908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ONLY CNV cases\n",
    "train_df = train_df[train_df['label']=='CNV']\n",
    "# get tf training set\n",
    "tf_train_set = get_training_tfdata(train_df,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d1b68",
   "metadata": {},
   "source": [
    "### Create output directory under `model_registry` to save training artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join('/model_registry/output',datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_cvae_subpixel_kl_reco_feat_loss_CNV')\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10936a83",
   "metadata": {},
   "source": [
    "### Sumsample a test batch to visualize the generation while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_batch in tf_train_set.take(1):\n",
    "    test_sample = test_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734384a",
   "metadata": {},
   "source": [
    "### Utility function to save generated images over the test batch while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_sample,apply_sigmo=False):\n",
    "    x_images_test, x_cond_labels_test = test_sample\n",
    "    mean, logvar = model.encode([x_images_test, x_cond_labels_test])\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "#     print(z.shape)\n",
    "#     print(z)\n",
    "#     print(mean,logvar)\n",
    "    predictions = model.decode([z, x_cond_labels_test],apply_sigmoid=apply_sigmo)\n",
    "    \n",
    "#     predictions = model.sample(z)\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        ax = plt.subplot(4, 4, i*2 + 1)\n",
    "        # plt.imshow(x_images_test[i, :, :, :])\n",
    "        plt.imshow((x_images_test[i, :, :, :] + 1) / 2)\n",
    "        ax.set_title(\"Real\")\n",
    "        plt.axis('off')\n",
    "        ax = plt.subplot(4, 4, i*2 + 2)\n",
    "        # plt.imshow(predictions[i, :, :, :])\n",
    "        plt.imshow((predictions[i, :, :, :] + 1) / 2)\n",
    "        ax.set_title(\"Predictions\")\n",
    "        plt.axis('off')\n",
    "\n",
    "      # tight_layout minimizes the overlap between 2 sub-plots\n",
    "    if apply_sigmo:\n",
    "        plt.savefig(LOG_DIR +'/image_at_epoch_{:04d}_sigmoid.png'.format(epoch))\n",
    "    else:\n",
    "        plt.savefig(LOG_DIR +'/image_at_epoch_{:04d}_nosigmoid.png'.format(epoch))\n",
    "    # plt.show()\n",
    "    plt.cla()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85d2f1",
   "metadata": {},
   "source": [
    "### Define CVAE model to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb72880",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CCVAE(image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee999d",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc55a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "or epoch in range(0, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    for train_x in tqdm(tf_train_set,\"running training\"):\n",
    "        tmp_x_images, tmp_x_labels = train_x\n",
    "        train_step(model, tmp_x_images, tmp_x_labels, optimizer)\n",
    "    end_time = time.time()\n",
    "\n",
    "    display.clear_output(wait=False)\n",
    "    generate_and_save_images(model, epoch, test_sample)\n",
    "    # generate_and_save_images(model, epoch, test_sample,apply_sigmo=True)\n",
    "    #save model\n",
    "    if epoch % 20==0:\n",
    "        filepath_encoder=os.path.join(LOG_DIR,\"model_encoder_\"+str(epoch)+\".h5\")\n",
    "        filepath_decoder=os.path.join(LOG_DIR,\"model_decoder_\"+str(epoch)+\".h5\")\n",
    "        model.encoder.save_weights(filepath_encoder)\n",
    "        model.decoder.save_weights(filepath_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93efa4",
   "metadata": {},
   "source": [
    "### Post-training data generation sample (over training set)\n",
    "\n",
    "This section demonstrates a way to generate images with the trained model. One may apply it over test set or any OCT sample with embbedings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set embedding path( the embeddings are extracted from contrastive_model_training)\n",
    "EMBEDDING_PATH = \"/data/processed/contrastive_learning/train_embeddings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save paths for real and corresponding real cases\n",
    "SAVE_PATH = \"/data/processed/cvae_train/train_embeddings/CNV/\"\n",
    "SAVE_DIR_REAL = os.path.join(SAVE_PATH,\"real\")\n",
    "SAVE_DIR_PRED = os.path.join(SAVE_PATH,\"pred\")\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH,exist_ok=True)\n",
    "    os.makedirs(SAVE_DIR_REAL,exist_ok=True)\n",
    "    os.makedirs(SAVE_DIR_PRED,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cec902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataloader.contrastive_learning_loader import _denorm\n",
    "from src.utils.cvae_utils import read_image_3_channel, load_np_embedding_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0105ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(my_model, x_images_test, x_cond_labels_test,apply_sigmo=False):\n",
    "    mean, logvar = my_model.encode([x_images_test, x_cond_labels_test])\n",
    "    z = my_model.reparameterize(mean, logvar)\n",
    "    predictions = my_model.decode([z, x_cond_labels_test],apply_sigmoid=apply_sigmo)[0]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples\n",
    "for indx,row in tqdm(train_df.iterrows()):\n",
    "    #read image \n",
    "    pre_procc_img = read_image_3_channel(row['path'])\n",
    "    pre_procc_img_2 = np.expand_dims(pre_procc_img,axis=0)\n",
    "    #read embedding \n",
    "    tmp_embedding_path = os.path.join(EMBEDDING_PATH,os.path.basename(row['path'])+\".npy\")\n",
    "    tmp_embedding = load_np_embedding_feature(tmp_embedding_path)\n",
    "    emb_exp_dim = np.expand_dims(tmp_embedding,axis=0)\n",
    "    # predict\n",
    "    tmp_pred = generate_and_save_images(model, pre_procc_img_2, emb_exp_dim,apply_sigmo=False)\n",
    "    tmp_pred_np = tmp_pred.numpy()\n",
    "    #save predicted and original \n",
    "    tmp_pred_np_denorm = _denorm(tmp_pred_np, np.min(tmp_pred_np), np.max(tmp_pred_np))\n",
    "    denorm_img_to_save = _denorm(pre_procc_img, np.min(pre_procc_img), np.max(pre_procc_img))\n",
    "    #save\n",
    "    tmp_basename = os.path.basename(row['path'])\n",
    "    cv2.imwrite(os.path.join(SAVE_DIR_REAL,tmp_basename),denorm_img_to_save*255,[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "    cv2.imwrite(os.path.join(SAVE_DIR_PRED,tmp_basename),tmp_pred_np_denorm*255,[cv2.IMWRITE_JPEG_QUALITY, 100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
